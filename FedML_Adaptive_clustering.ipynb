{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yu92vxJbKN2I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTvNLQ4SLP4-",
        "outputId": "8831af3d-26b8-424b-a325-4fbc4e0a365f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtgQda39LP1k",
        "outputId": "81450336-4551-4410-e44f-58f7fba445f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-cacbf28de427>:1: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train=pd.read_csv('/content/drive/MyDrive/CIC-IDS-2018(15).csv')\n"
          ]
        }
      ],
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/CIC-IDS-2018(15).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5rVoyzdvKWGJ"
      },
      "outputs": [],
      "source": [
        "train.drop(train.loc[train[\"Label\"] == \"Label\"].index, inplace=True)\n",
        "train[\"Protocol\"].unique()\n",
        "train = train.astype({\"Protocol\": str})\n",
        "train[\"Protocol\"].unique()\n",
        "train = pd.get_dummies(train, columns=['Protocol'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LvjFtdyHJ41T"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\n",
        "    'Dst Port',\n",
        "    'Timestamp',\n",
        "    'Fwd PSH Flags',\n",
        "    'Bwd PSH Flags',\n",
        "    'Fwd URG Flags',\n",
        "    'Bwd URG Flags',\n",
        "    'Flow Byts/s',\n",
        "    'Flow Pkts/s'\n",
        "]\n",
        "train.drop(columns=columns_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ls2ODu5uJ4yz"
      },
      "outputs": [],
      "source": [
        "train.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qEgIWojIJ4wb"
      },
      "outputs": [],
      "source": [
        "train.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "htTMJJIpJ4t8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "train['Label'] = encoder.fit_transform(train['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "prd8Xie-J4rc"
      },
      "outputs": [],
      "source": [
        "train = pd.DataFrame(train)\n",
        "train.replace([np.inf, -np.inf], np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3QheEoTcgdsQ"
      },
      "outputs": [],
      "source": [
        "train.to_csv('/content/drive/MyDrive/cicids_preprocessed_15.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CMKKP2jDO6Dz"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/cicids_preprocessed_15.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "features = df.drop([\"Label\"], axis=1)\n",
        "labels =  df[\"Label\"]\n",
        "\n",
        "le = LabelEncoder()\n",
        "target_class = labels.unique()\n",
        "labels = le.fit_transform( labels )\n",
        "X=features\n",
        "y=labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Mpq6dgcyMIJZ"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P3CjHL8eO6BD"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gtdz_zs_NFm2"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(73, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 14)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W_FeZaKAM4x4"
      },
      "outputs": [],
      "source": [
        "def perform_clustering(model, stacked_params, n_clusters):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(stacked_params.detach().numpy())\n",
        "    return cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xZUc1752M6mJ"
      },
      "outputs": [],
      "source": [
        "def select_clients(train_loaders, cluster_labels):\n",
        "    selected_clusters = set()\n",
        "    selected_clients = []\n",
        "    for loader, label in zip(train_loaders, cluster_labels):\n",
        "        if label not in selected_clusters:\n",
        "            selected_clusters.add(label)\n",
        "            selected_clients.append(loader)\n",
        "    return selected_clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dbr4ducOM1xo"
      },
      "outputs": [],
      "source": [
        "def federated_train_with_sa_like_clustering(model, train_loaders, test_loader, num_epochs=5, lr=0.001, threshold_w=0.5, initial_p=8, max_p=8, stabilize_rounds=3, sa_prob=0.4):\n",
        "    global_model = model\n",
        "    global_optimizer = optim.Adam(global_model.parameters(), lr=lr)\n",
        "    p = initial_p\n",
        "    d = 1\n",
        "    prev_loss = float('inf')\n",
        "    stabilize_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        global_model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        local_params_list = []\n",
        "\n",
        "        for train_loader in train_loaders:\n",
        "            local_model = SimpleMLP()\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "            local_optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
        "\n",
        "            for data, target in train_loader:\n",
        "                local_optimizer.zero_grad()\n",
        "                output = local_model(data)\n",
        "                loss = nn.functional.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                local_optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            local_model.load_state_dict(local_model.state_dict())\n",
        "            local_params = torch.cat([param.data.view(-1) for param in local_model.parameters()])\n",
        "            local_params_list.append(local_params.unsqueeze(0))\n",
        "            stacked_params = torch.cat(local_params_list, dim=0)\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        reduction_ratio = prev_loss / avg_loss if prev_loss > 0 else 0\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}: Avg Loss: {avg_loss:.4f}, Reduction Ratio: {reduction_ratio:.4f}, Threshold: {threshold_w}, p: {p}, d: {d}\")\n",
        "\n",
        "        if reduction_ratio > threshold_w:\n",
        "            # SA-like algorithm: Decide whether to keep p unchanged based on sa_prob\n",
        "            if random.random() < sa_prob:\n",
        "                # Keep the number of clusters unchanged\n",
        "                stabilize_count += 1\n",
        "            else:\n",
        "                # Decrease the number of clusters\n",
        "                stabilize_count = 0\n",
        "                p = max(p - d, 1)\n",
        "                d = min(d + 1, max_p - 1)  # Increment d\n",
        "                print(f\"Adjusting clusters DOWN. New number of clusters: {p}, d: {d}\")\n",
        "        else:\n",
        "            stabilize_count += 1\n",
        "\n",
        "        if stabilize_count >= stabilize_rounds:\n",
        "            # Reset d and stabilize count if we've stabilized for enough rounds\n",
        "            d = 1\n",
        "            stabilize_count = 0\n",
        "\n",
        "        prev_loss = avg_loss\n",
        "\n",
        "        cluster_labels = perform_clustering(global_model, stacked_params, n_clusters=p)\n",
        "\n",
        "        selected_clients = select_clients(train_loaders, cluster_labels)\n",
        "\n",
        "        for train_loader in selected_clients:\n",
        "            local_model = SimpleMLP()\n",
        "            local_model.load_state_dict(global_model.state_dict())\n",
        "            local_optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
        "\n",
        "            for data, target in train_loader:\n",
        "                local_optimizer.zero_grad()\n",
        "                output = local_model(data)\n",
        "                loss = nn.functional.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                local_optimizer.step()\n",
        "\n",
        "            global_params = global_model.state_dict()\n",
        "            local_params = local_model.state_dict()\n",
        "            for key in global_params.keys():\n",
        "                global_params[key] += local_params[key] / len(selected_clients)\n",
        "\n",
        "        global_model.load_state_dict(global_params)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                outputs = global_model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} completed. Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return global_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VWBCY_3ILPyv"
      },
      "outputs": [],
      "source": [
        "num_clients = 8\n",
        "train_loaders = [DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True) for _ in range(num_clients)]\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk9s-v9HLPwI",
        "outputId": "c4c590af-6a63-492c-fda0-0bf0647b7268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7: Avg Loss: 0.0904, Reduction Ratio: inf, Threshold: 0.5, p: 8, d: 1\n",
            "Adjusting clusters DOWN. New number of clusters: 7, d: 2\n",
            "Epoch 1/7 completed. Accuracy: 98.20%\n",
            "Epoch 2/7: Avg Loss: 0.0293, Reduction Ratio: 3.0843, Threshold: 0.5, p: 7, d: 2\n",
            "Adjusting clusters DOWN. New number of clusters: 5, d: 3\n",
            "Epoch 2/7 completed. Accuracy: 98.12%\n",
            "Epoch 3/7: Avg Loss: 0.0381, Reduction Ratio: 0.7696, Threshold: 0.5, p: 5, d: 3\n",
            "Epoch 3/7 completed. Accuracy: 98.45%\n",
            "Epoch 4/7: Avg Loss: 0.1968, Reduction Ratio: 0.1936, Threshold: 0.5, p: 5, d: 3\n",
            "Epoch 4/7 completed. Accuracy: 99.00%\n",
            "Epoch 5/7: Avg Loss: 1.5148, Reduction Ratio: 0.1299, Threshold: 0.5, p: 5, d: 3\n",
            "Epoch 5/7 completed. Accuracy: 99.40%\n",
            "Epoch 6/7: Avg Loss: 15.6997, Reduction Ratio: 0.0965, Threshold: 0.5, p: 5, d: 1\n",
            "Epoch 6/7 completed. Accuracy: 99.60%\n",
            "Epoch 7/7: Avg Loss: 197.4713, Reduction Ratio: 0.0795, Threshold: 0.5, p: 5, d: 1\n",
            "Epoch 7/7 completed. Accuracy: 99.67%\n"
          ]
        }
      ],
      "source": [
        "global_model = SimpleMLP()\n",
        "trained_model = federated_train_with_sa_like_clustering(global_model, train_loaders, test_loader, num_epochs=7, initial_p=num_clients, max_p=num_clients, stabilize_rounds=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
